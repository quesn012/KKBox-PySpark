{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " # Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we need to load the data as a CSV. Note I also showed a row for simplicity to understand what we're looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    ".option(\"header\",True) \\\n",
    ".option(\"inferSchema\",True) \\\n",
    ".load('file:/home/cloudera/Big Data Project/final_with_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>days_used</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_num_25</th>\n",
       "      <th>total_num_50</th>\n",
       "      <th>total_num_75</th>\n",
       "      <th>total_num_985</th>\n",
       "      <th>total_num_100</th>\n",
       "      <th>avg_unique_songs</th>\n",
       "      <th>avg_total_secs</th>\n",
       "      <th>avg_plan_length</th>\n",
       "      <th>avg_expected_plan_price</th>\n",
       "      <th>avg_actual_plan_price</th>\n",
       "      <th>max_expiration_date</th>\n",
       "      <th>min_transaction_date</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>++0wqjjQge1mBBe5r4ciHGKwtF/m322zkra7CK8I+Mw=</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>20170306</td>\n",
       "      <td>20151107</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id  days_used  \\\n",
       "0  ++0wqjjQge1mBBe5r4ciHGKwtF/m322zkra7CK8I+Mw=          0   \n",
       "\n",
       "   total_transactions total_num_25 total_num_50 total_num_75 total_num_985  \\\n",
       "0                  16           \\N           \\N           \\N            \\N   \n",
       "\n",
       "  total_num_100 avg_unique_songs avg_total_secs  avg_plan_length  \\\n",
       "0            \\N               \\N             \\N             30.0   \n",
       "\n",
       "   avg_expected_plan_price  avg_actual_plan_price  max_expiration_date  \\\n",
       "0                     99.0                   99.0             20170306   \n",
       "\n",
       "   min_transaction_date city age gender registered_via  is_churn  \n",
       "0              20151107   \\N  \\N     \\N             \\N         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then we want to check the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- days_used: integer (nullable = true)\n",
      " |-- total_transactions: integer (nullable = true)\n",
      " |-- total_num_25: string (nullable = true)\n",
      " |-- total_num_50: string (nullable = true)\n",
      " |-- total_num_75: string (nullable = true)\n",
      " |-- total_num_985: string (nullable = true)\n",
      " |-- total_num_100: string (nullable = true)\n",
      " |-- avg_unique_songs: string (nullable = true)\n",
      " |-- avg_total_secs: string (nullable = true)\n",
      " |-- avg_plan_length: double (nullable = true)\n",
      " |-- avg_expected_plan_price: double (nullable = true)\n",
      " |-- avg_actual_plan_price: double (nullable = true)\n",
      " |-- max_expiration_date: integer (nullable = true)\n",
      " |-- min_transaction_date: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- registered_via: string (nullable = true)\n",
      " |-- is_churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice that sum of the numbers came in as strings due to NA values. We're going to have to clean next so there are 2 steps to this:\n",
    "\n",
    "1. Filling \\N's in the appropriate columsns with 0. We will also want to convert the strings in these fields to floats.\n",
    "    - these columns are\n",
    "2. Dropping rows that aren't relevant because they have missing data.\n",
    "    - these columns are: city, age, gender, registered via\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's also get a row count so we have some idea of proportionality moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992931"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('user_id').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before starting 1, lets check how many are missing from these columns. Note that I assume if one column is missing they are all misssing as they all were pulled from the same table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|total_num_25| count|\n",
      "+------------+------+\n",
      "|          \\N|123005|\n",
      "|           0|  2684|\n",
      "|          36|   845|\n",
      "|          72|   776|\n",
      "|          60|   751|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('total_num_25').groupby('total_num_25').count().sort('count', ascending = False).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So we have 123k missing the values pulled from the usage. That being said, since these users were present in other tables I'm going to make the assumption they were registered but for whatever reason they did not use their account actively. In this case, we will leave them in, but fill these usage related values as 0's in our next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "data = data.withColumn('total_num_25', \\\n",
    "                      when(data['total_num_25']=='\\N', '0.0').otherwise(data['total_num_25']))\n",
    "\n",
    "data = data.withColumn('total_num_50', \\\n",
    "                      when(data['total_num_50']=='\\N', '0.0').otherwise(data['total_num_50']))\n",
    "\n",
    "data = data.withColumn('total_num_75', \\\n",
    "                      when(data['total_num_75']=='\\N', '0.0').otherwise(data['total_num_75']))\n",
    "\n",
    "data = data.withColumn('total_num_985', \\\n",
    "                      when(data['total_num_985']=='\\N', '0.0').otherwise(data['total_num_985']))\n",
    "\n",
    "data = data.withColumn('total_num_100', \\\n",
    "                      when(data['total_num_100']=='\\N', '0.0').otherwise(data['total_num_100']))\n",
    "\n",
    "data = data.withColumn('avg_unique_songs', \\\n",
    "                      when(data['avg_unique_songs']=='\\N', '0.0').otherwise(data['avg_unique_songs']))\n",
    "\n",
    "data = data.withColumn('avg_total_secs', \\\n",
    "                      when(data['avg_total_secs']=='\\N', '0.0').otherwise(data['avg_total_secs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Checking the first row to make sure the above corrections worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>days_used</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_num_25</th>\n",
       "      <th>total_num_50</th>\n",
       "      <th>total_num_75</th>\n",
       "      <th>total_num_985</th>\n",
       "      <th>total_num_100</th>\n",
       "      <th>avg_unique_songs</th>\n",
       "      <th>avg_total_secs</th>\n",
       "      <th>avg_plan_length</th>\n",
       "      <th>avg_expected_plan_price</th>\n",
       "      <th>avg_actual_plan_price</th>\n",
       "      <th>max_expiration_date</th>\n",
       "      <th>min_transaction_date</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>++0wqjjQge1mBBe5r4ciHGKwtF/m322zkra7CK8I+Mw=</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>20170306</td>\n",
       "      <td>20151107</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id  days_used  \\\n",
       "0  ++0wqjjQge1mBBe5r4ciHGKwtF/m322zkra7CK8I+Mw=          0   \n",
       "\n",
       "   total_transactions total_num_25 total_num_50 total_num_75 total_num_985  \\\n",
       "0                  16          0.0          0.0          0.0           0.0   \n",
       "\n",
       "  total_num_100 avg_unique_songs avg_total_secs  avg_plan_length  \\\n",
       "0           0.0              0.0            0.0             30.0   \n",
       "\n",
       "   avg_expected_plan_price  avg_actual_plan_price  max_expiration_date  \\\n",
       "0                     99.0                   99.0             20170306   \n",
       "\n",
       "   min_transaction_date city age gender registered_via  is_churn  \n",
       "0              20151107   \\N  \\N     \\N             \\N         0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we will try to drop rows missing some values... (city, age, gender, registered via) as these are important attributes we want in our analysis. First lets see how many rows are missing these values; note taht i assumed if its missing for city its missing for gender/age/registered via as well since they all pulled from the same table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age| count|\n",
      "+---+------+\n",
      "|  0|433567|\n",
      "| \\N|296297|\n",
      "| 27| 15344|\n",
      "| 26| 14611|\n",
      "| 25| 13778|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('age').groupby('age').count().sort('count', ascending = False).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "296,297 is a little more than a quarter of rows misssing these values; its a substantial number but since these attributes were from their customer table, I find it a bit odd that the customer table is missing this data while the payment table still has it. From earlier analysis, I could also see a lot of these rows were also missing usage data. Since we are testing the capabilities of pyspark, I am going to make the executive decision to drop these rows; we will still have a viable model built off a large quantity of data without them and will be able to drive better insights with the data not being so present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = data.filter(data.age != '\\N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>days_used</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_num_25</th>\n",
       "      <th>total_num_50</th>\n",
       "      <th>total_num_75</th>\n",
       "      <th>total_num_985</th>\n",
       "      <th>total_num_100</th>\n",
       "      <th>avg_unique_songs</th>\n",
       "      <th>avg_total_secs</th>\n",
       "      <th>avg_plan_length</th>\n",
       "      <th>avg_expected_plan_price</th>\n",
       "      <th>avg_actual_plan_price</th>\n",
       "      <th>max_expiration_date</th>\n",
       "      <th>min_transaction_date</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>++Bks8kE9oclzxZM3hcWs+qzsxuoXFeIE1+7pxKBCQg=</td>\n",
       "      <td>624</td>\n",
       "      <td>624</td>\n",
       "      <td>2408</td>\n",
       "      <td>312</td>\n",
       "      <td>264</td>\n",
       "      <td>360</td>\n",
       "      <td>18864</td>\n",
       "      <td>20.76923</td>\n",
       "      <td>6836.1167</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>20170331</td>\n",
       "      <td>20160730</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id  days_used  \\\n",
       "0  ++Bks8kE9oclzxZM3hcWs+qzsxuoXFeIE1+7pxKBCQg=        624   \n",
       "\n",
       "   total_transactions total_num_25 total_num_50 total_num_75 total_num_985  \\\n",
       "0                 624         2408          312          264           360   \n",
       "\n",
       "  total_num_100 avg_unique_songs avg_total_secs  avg_plan_length  \\\n",
       "0         18864         20.76923      6836.1167             30.0   \n",
       "\n",
       "   avg_expected_plan_price  avg_actual_plan_price  max_expiration_date  \\\n",
       "0                     99.0                   99.0             20170331   \n",
       "\n",
       "   min_transaction_date city age gender registered_via  is_churn  \n",
       "0              20160730    1   0                     7         0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since we have some null values in gender, we convert this to a new string called 'Not_Provided' so our model can still use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|      gender| count|\n",
      "+------------+------+\n",
      "|Not_Provided|430471|\n",
      "|        male|141045|\n",
      "|      female|125118|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('gender',when(\n",
    "data['gender']=='', 'Not_Provided').otherwise(data['gender']))\n",
    "\n",
    "data.select('gender').groupby('gender').count().sort('count', ascending = False).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The final thing we have to do in our data munging process is find the date difference between the max_expiration_date and the min_transaction_date.\n",
    "\n",
    "First we need to convert both to dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- days_used: integer (nullable = true)\n",
      " |-- total_transactions: integer (nullable = true)\n",
      " |-- total_num_25: string (nullable = true)\n",
      " |-- total_num_50: string (nullable = true)\n",
      " |-- total_num_75: string (nullable = true)\n",
      " |-- total_num_985: string (nullable = true)\n",
      " |-- total_num_100: string (nullable = true)\n",
      " |-- avg_unique_songs: string (nullable = true)\n",
      " |-- avg_total_secs: string (nullable = true)\n",
      " |-- avg_plan_length: double (nullable = true)\n",
      " |-- avg_expected_plan_price: double (nullable = true)\n",
      " |-- avg_actual_plan_price: double (nullable = true)\n",
      " |-- max_expiration_date: string (nullable = true)\n",
      " |-- min_transaction_date: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- registered_via: string (nullable = true)\n",
      " |-- is_churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('max_expiration_date',data.max_expiration_date.cast(\"string\"))\n",
    "data = data.withColumn('min_transaction_date',data.min_transaction_date.cast(\"string\"))\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|max_expiration_date|min_transaction_date|\n",
      "+-------------------+--------------------+\n",
      "|           20170331|            20160730|\n",
      "|           20170311|            20150914|\n",
      "|           20170316|            20150131|\n",
      "|           20170331|            20150121|\n",
      "|           20170305|            20150410|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.registerTempTable('data')\n",
    "\n",
    "data.select('max_expiration_date','min_transaction_date').limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to use SqlContext to convert each date to a date type colum using SQL Context. We also test to make sure the conversion worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#building the correct date vectors\n",
    "max_exp_date = sqlContext.sql(\"\"\"\n",
    "    SELECT TO_DATE(CAST(UNIX_TIMESTAMP(max_expiration_date,'yyyyMMdd') AS TIMESTAMP)) AS max_exp_date\n",
    "    FROM DATA\n",
    "\"\"\")\n",
    "\n",
    "min_trans_date = sqlContext.sql(\"\"\"\n",
    "    SELECT TO_DATE(CAST(UNIX_TIMESTAMP(min_transaction_date,'yyyyMMdd') AS TIMESTAMP)) AS min_trans_date\n",
    "    FROM DATA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|max_exp_date|\n",
      "+------------+\n",
      "|  2017-03-31|\n",
      "|  2017-03-11|\n",
      "|  2017-03-16|\n",
      "|  2017-03-31|\n",
      "|  2017-03-05|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_exp_date.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|min_trans_date|\n",
      "+--------------+\n",
      "|    2016-07-30|\n",
      "|    2015-09-14|\n",
      "|    2015-01-31|\n",
      "|    2015-01-21|\n",
      "|    2015-04-10|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_trans_date.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we merge these new date type dates back into our original data frame, then calculate the date difference as a new column titled 'date_diff'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#merging these into the data dataFrame\n",
    "\n",
    "data = data.withColumn('row_index', monotonically_increasing_id())\n",
    "max_exp_date = max_exp_date.withColumn('row_index', monotonically_increasing_id())\n",
    "min_trans_date = min_trans_date.withColumn('row_index', monotonically_increasing_id())\n",
    "\n",
    "data = data.join(max_exp_date, on=['row_index'])\n",
    "data = data.join(min_trans_date, on=['row_index']).drop('row_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#make date diff column\n",
    "data = data.withColumn('date_diff', datediff(data.max_exp_date,data.min_trans_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>days_used</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_num_25</th>\n",
       "      <th>total_num_50</th>\n",
       "      <th>total_num_75</th>\n",
       "      <th>total_num_985</th>\n",
       "      <th>total_num_100</th>\n",
       "      <th>avg_unique_songs</th>\n",
       "      <th>avg_total_secs</th>\n",
       "      <th>...</th>\n",
       "      <th>max_expiration_date</th>\n",
       "      <th>min_transaction_date</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>is_churn</th>\n",
       "      <th>max_exp_date</th>\n",
       "      <th>min_trans_date</th>\n",
       "      <th>date_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+1ZgRw2ZlmD4Z1NCVo8lh4ECpNtG73bp/cECvhq4l8Q=</td>\n",
       "      <td>1071</td>\n",
       "      <td>1071</td>\n",
       "      <td>2961</td>\n",
       "      <td>1050</td>\n",
       "      <td>735</td>\n",
       "      <td>651</td>\n",
       "      <td>16212</td>\n",
       "      <td>16.745098</td>\n",
       "      <td>4050.471</td>\n",
       "      <td>...</td>\n",
       "      <td>20170311</td>\n",
       "      <td>20150802</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-11</td>\n",
       "      <td>2015-08-02</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+NKVPkGwpoOWKQDdH3mtpaZGR5lx9fu5bOHixIRjsnI=</td>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "      <td>17385</td>\n",
       "      <td>3495</td>\n",
       "      <td>3270</td>\n",
       "      <td>2580</td>\n",
       "      <td>75045</td>\n",
       "      <td>18.718391</td>\n",
       "      <td>8045.0156</td>\n",
       "      <td>...</td>\n",
       "      <td>20170315</td>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+i2T+lq7TNR/gThVOEh6M3CdHEIgZhgeH1ENTjAgMyE=</td>\n",
       "      <td>6403</td>\n",
       "      <td>6403</td>\n",
       "      <td>39349</td>\n",
       "      <td>10260</td>\n",
       "      <td>6175</td>\n",
       "      <td>5966</td>\n",
       "      <td>79591</td>\n",
       "      <td>19.973293</td>\n",
       "      <td>3643.7224</td>\n",
       "      <td>...</td>\n",
       "      <td>20170319</td>\n",
       "      <td>20150920</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-19</td>\n",
       "      <td>2015-09-20</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/67f8zgh70yyzqwntxaAuqqSrbibNC7KxG5rGBg4/hc=</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>390</td>\n",
       "      <td>55</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>1970</td>\n",
       "      <td>12.230769</td>\n",
       "      <td>4521.8457</td>\n",
       "      <td>...</td>\n",
       "      <td>20170331</td>\n",
       "      <td>20161029</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>2016-10-29</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/UYp4Ued/yMVEf5OD13C9Hz8B/N78PBx13tglE3+gXA=</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>213</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2409</td>\n",
       "      <td>19.564102</td>\n",
       "      <td>5146.8057</td>\n",
       "      <td>...</td>\n",
       "      <td>20170331</td>\n",
       "      <td>20161231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id  days_used  \\\n",
       "0  +1ZgRw2ZlmD4Z1NCVo8lh4ECpNtG73bp/cECvhq4l8Q=       1071   \n",
       "1  +NKVPkGwpoOWKQDdH3mtpaZGR5lx9fu5bOHixIRjsnI=       2610   \n",
       "2  +i2T+lq7TNR/gThVOEh6M3CdHEIgZhgeH1ENTjAgMyE=       6403   \n",
       "3  /67f8zgh70yyzqwntxaAuqqSrbibNC7KxG5rGBg4/hc=        130   \n",
       "4  /UYp4Ued/yMVEf5OD13C9Hz8B/N78PBx13tglE3+gXA=        117   \n",
       "\n",
       "   total_transactions total_num_25 total_num_50 total_num_75 total_num_985  \\\n",
       "0                1071         2961         1050          735           651   \n",
       "1                2610        17385         3495         3270          2580   \n",
       "2                6403        39349        10260         6175          5966   \n",
       "3                 130          390           55           80            65   \n",
       "4                 117          213           21            9            15   \n",
       "\n",
       "  total_num_100 avg_unique_songs avg_total_secs    ...      \\\n",
       "0         16212        16.745098       4050.471    ...       \n",
       "1         75045        18.718391      8045.0156    ...       \n",
       "2         79591        19.973293      3643.7224    ...       \n",
       "3          1970        12.230769      4521.8457    ...       \n",
       "4          2409        19.564102      5146.8057    ...       \n",
       "\n",
       "   max_expiration_date  min_transaction_date  city age        gender  \\\n",
       "0             20170311              20150802     1   0  Not_Provided   \n",
       "1             20170315              20151216     1   0  Not_Provided   \n",
       "2             20170319              20150920     1   0  Not_Provided   \n",
       "3             20170331              20161029     1   0  Not_Provided   \n",
       "4             20170331              20161231     1   0  Not_Provided   \n",
       "\n",
       "  registered_via is_churn max_exp_date min_trans_date  date_diff  \n",
       "0              7        0   2017-03-11     2015-08-02        587  \n",
       "1              7        0   2017-03-15     2015-12-16        455  \n",
       "2              7        0   2017-03-19     2015-09-20        546  \n",
       "3              7        0   2017-03-31     2016-10-29        153  \n",
       "4              7        0   2017-03-31     2016-12-31         90  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model Prep and Pipeline Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next up we need to get our data in a format that can be fed to models. First, we will select the data and also cast it to its appropriate data types now that its been cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- days_used: integer (nullable = true)\n",
      " |-- total_transactions: integer (nullable = true)\n",
      " |-- total_num_25: string (nullable = true)\n",
      " |-- total_num_50: string (nullable = true)\n",
      " |-- total_num_75: string (nullable = true)\n",
      " |-- total_num_985: string (nullable = true)\n",
      " |-- total_num_100: string (nullable = true)\n",
      " |-- avg_unique_songs: string (nullable = true)\n",
      " |-- avg_total_secs: string (nullable = true)\n",
      " |-- avg_plan_length: double (nullable = true)\n",
      " |-- avg_expected_plan_price: double (nullable = true)\n",
      " |-- avg_actual_plan_price: double (nullable = true)\n",
      " |-- max_expiration_date: string (nullable = true)\n",
      " |-- min_transaction_date: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- registered_via: string (nullable = true)\n",
      " |-- is_churn: integer (nullable = true)\n",
      " |-- max_exp_date: date (nullable = true)\n",
      " |-- min_trans_date: date (nullable = true)\n",
      " |-- date_diff: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days_used</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_num_25</th>\n",
       "      <th>total_num_50</th>\n",
       "      <th>total_num_75</th>\n",
       "      <th>total_num_985</th>\n",
       "      <th>total_num_100</th>\n",
       "      <th>avg_unique_songs</th>\n",
       "      <th>avg_total_secs</th>\n",
       "      <th>avg_plan_length</th>\n",
       "      <th>avg_expected_plan_price</th>\n",
       "      <th>avg_actual_plan_price</th>\n",
       "      <th>date_diff</th>\n",
       "      <th>age</th>\n",
       "      <th>city</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1071.0</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>2961.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>16212.0</td>\n",
       "      <td>16.745098</td>\n",
       "      <td>4050.471</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.04762</td>\n",
       "      <td>99.04762</td>\n",
       "      <td>587.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   days_used  total_transactions  total_num_25  total_num_50  total_num_75  \\\n",
       "0     1071.0              1071.0        2961.0        1050.0         735.0   \n",
       "\n",
       "   total_num_985  total_num_100  avg_unique_songs  avg_total_secs  \\\n",
       "0          651.0        16212.0         16.745098        4050.471   \n",
       "\n",
       "   avg_plan_length  avg_expected_plan_price  avg_actual_plan_price  date_diff  \\\n",
       "0             30.0                 99.04762               99.04762      587.0   \n",
       "\n",
       "   age city        gender registered_via  is_churn  \n",
       "0  0.0    1  Not_Provided              7       0.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = data.select(data.days_used.cast('double'),\n",
    "                   data.total_transactions.cast('double'),\n",
    "                   data.total_num_25.cast('double'),\n",
    "                   data.total_num_50.cast('double'),\n",
    "                   data.total_num_75.cast('double'),\n",
    "                   data.total_num_985.cast('double'),\n",
    "                   data.total_num_100.cast('double'),\n",
    "                   data.avg_unique_songs.cast('double'),\n",
    "                   data.avg_total_secs.cast('double'),\n",
    "                   data.avg_plan_length.cast('double'),\n",
    "                   data.avg_expected_plan_price.cast('double'),\n",
    "                   data.avg_actual_plan_price.cast('double'),\n",
    "                   data.date_diff.cast('double'),\n",
    "                   data.age.cast('double'),\n",
    "                   data.city,\n",
    "                   data.gender,\n",
    "                   data.registered_via,\n",
    "                   data.is_churn.cast('double')               \n",
    "                  )\n",
    "\n",
    "cols.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Indexing categorical variables, then encoding them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler, VectorIndexer, OneHotEncoder, StringIndexer)\n",
    "\n",
    "#City, gender, registered via\n",
    "#Creating Indexers\n",
    "city_indexer = StringIndexer(inputCol='city', outputCol='cityIndex')\n",
    "gender_indexer = StringIndexer(inputCol='gender', outputCol='genderIndex')\n",
    "registered_via_indexer = StringIndexer(inputCol='registered_via', outputCol='registered_viaIndex')\n",
    "churn_indexer = StringIndexer(inputCol='is_churn', outputCol='is_churnIndex')\n",
    "\n",
    "#Creating Encoders\n",
    "city_encoder = OneHotEncoder(inputCol='cityIndex', outputCol='cityVec')\n",
    "gender_encoder = OneHotEncoder(inputCol='genderIndex', outputCol='genderVec')\n",
    "registered_via_encoder = OneHotEncoder(inputCol='registered_viaIndex', outputCol='registered_viaVec')\n",
    "churn_encoder = OneHotEncoder(inputCol='is_churnIndex', outputCol='is_churnVec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Building Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['days_used','total_transactions','total_num_25','total_num_50','total_num_75',\n",
    "                                      'total_num_985','total_num_100','avg_unique_songs','avg_total_secs','avg_plan_length',\n",
    "                                      'avg_expected_plan_price','avg_actual_plan_price','date_diff','age','cityVec','genderVec',\n",
    "                                      'registered_viaVec'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Importing and setting up logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "log_reg = LogisticRegression(labelCol='is_churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Pipeline Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipe = Pipeline(stages=[city_indexer,\n",
    "                       gender_indexer,\n",
    "                       registered_via_indexer,\n",
    "                       city_encoder,\n",
    "                       gender_encoder,\n",
    "                       registered_via_encoder,\n",
    "                       assembler,\n",
    "                       log_reg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Splitting the data to a 70 / 30 - test / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = cols.randomSplit([.7,.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Fitting the Model to our Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fit = pipe.fit(train_data)\n",
    "\n",
    "#3 minutes 40 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "results = fit.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evaluating our Logistic Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_reg_model = fit.stages[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out the coeffecients for our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.76981774646e-06,-2.77028020459e-06,2.40386775847e-08,1.5243337119e-07,-7.29547587911e-08,-1.16132044599e-07,-3.24917953939e-08,0.00129359986098,3.28902230099e-17,0.00153692421394,0.000920840544739,0.000815303931357,-0.000113107106972,0.000706489031535,-0.077814969204,0.0380074112638,0.0418986711783,0.0474049215087,0.0576303332431,0.056205335389,0.0878063887289,0.0215013542602,0.0734439640239,0.0381708330171,0.0630016420282,0.048244453277,0.10650677688,0.0363828611859,0.00991158409444,0.074281895514,0.0432528166236,-0.00524575779075,0.0136111436158,0.0567259207853,-0.082177963689,0.0673396541995,-0.191555320707,0.0707969001526,0.220590090054,0.326696198719]\n"
     ]
    }
   ],
   "source": [
    "print log_reg_model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating our metrics (AUC and area under precision recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834534232775\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "data_eval = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='is_churn',)\n",
    "auc = data_eval.evaluate(results)#.fMeasureByThreshold\n",
    "print auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0399069265069\n"
     ]
    }
   ],
   "source": [
    "data_eval.setMetricName('areaUnderPR')\n",
    "\n",
    "areaUnderPrecisionRecall = data_eval.evaluate(results)\n",
    "\n",
    "print areaUnderPrecisionRecall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will do the same thing we did for logistic regression with Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aligned parameters with out other platforms to compare similar trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol='is_churnIndex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for the RF pipeline we have to use churn_indexer. RF in ML requise the target variable for categorical to be run through a string indexer in order to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline(stages=[city_indexer,\n",
    "                       gender_indexer,\n",
    "                       registered_via_indexer,\n",
    "                         churn_indexer,\n",
    "                       city_encoder,\n",
    "                       gender_encoder,\n",
    "                       registered_via_encoder,\n",
    "#                           churn_encoder,\n",
    "                       assembler,\n",
    "                       rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the RF to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rf_fit = rf_pipe.fit(train_data)\n",
    "\n",
    "#takes 5 minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on our testing data for our RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating our Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rf_results = rf_fit.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.861153257612\n"
     ]
    }
   ],
   "source": [
    "rf_data_eval = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='is_churnIndex')\n",
    "\n",
    "rf_auc = rf_data_eval.evaluate(rf_results)\n",
    "\n",
    "print rf_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0596329188101\n"
     ]
    }
   ],
   "source": [
    "rf_data_eval.setMetricName('areaUnderPR')\n",
    "\n",
    "rf_areaUnderPrecisionRecall = rf_data_eval.evaluate(rf_results)\n",
    "\n",
    "print rf_areaUnderPrecisionRecall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "All in all, our random forest produced better result than our logistic regression. It did take longer to run, but not a significant amount of time to make a difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
